import time
import json
import re
import math
import random
from datetime import datetime
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor

# --- GEREKLÄ° KÃœTÃœPHANELER ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# --- AYARLAR ---
BASE_URL = "https://www.vakifkart.com.tr"
LIST_URL_TEMPLATE = "https://www.vakifkart.com.tr/kampanyalar/sayfa/{}"
OUTPUT_FILE = "vakifbank_parallel_final.json"
IMPORT_SOURCE_NAME = "VakÄ±fBank World"
WORKER_COUNT = 4  # AynÄ± anda Ã§alÄ±ÅŸacak tarayÄ±cÄ± sayÄ±sÄ± (BilgisayarÄ±n gÃ¼cÃ¼ne gÃ¶re artÄ±rÄ±labilir)

# --- YARDIMCI FONKSÄ°YONLAR (Ziraat v31 Motoru) ---
def tr_lower(text):
    return text.replace('I', 'Ä±').replace('Ä°', 'i').lower()

def temizle_metin(text):
    if not text: return ""
    text = text.replace('\n', ' ').replace('\r', '')
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'^(SektÃ¶r:|Kampanya KoÅŸullarÄ±:|Kampanya DetaylarÄ±:)\s*', '', text, flags=re.IGNORECASE)
    return text.strip(' ,.:;-')

def format_rakam(rakam_int):
    if rakam_int is None: return None
    try: return f"{int(rakam_int):,}".replace(",", ".")
    except: return None

def format_tarih_iso(tarih_str, is_end=False):
    if not tarih_str: return None
    ts = tr_lower(tarih_str)
    aylar = {'ocak':'01','ÅŸubat':'02','mart':'03','nisan':'04','mayÄ±s':'05','haziran':'06',
             'temmuz':'07','aÄŸustos':'08','eylÃ¼l':'09','ekim':'10','kasÄ±m':'11','aralÄ±k':'12'}
    try:
        m = re.search(r'(\d{1,2})\s*-\s*(\d{1,2})\s*([a-zÄŸÃ¼ÅŸÄ±Ã¶Ã§]+)\s*(\d{4})', ts)
        if m:
            g1, g2, ay, yil = m.groups()
            g = g2 if is_end else g1
            h = "23:59:59" if is_end else "00:00:00"
            return f"{yil}-{aylar.get(ay,'01')}-{str(g).zfill(2)}T{h}Z"
        m2 = re.search(r'(\d{1,2})\s*([a-zÄŸÃ¼ÅŸÄ±Ã¶Ã§]+)\s*-\s*(\d{1,2})\s*([a-zÄŸÃ¼ÅŸÄ±Ã¶Ã§]+)\s*(\d{4})', ts)
        if m2:
            g1, a1, g2, a2, yil = m2.groups()
            if is_end: return f"{yil}-{aylar.get(a2,'12')}-{str(g2).zfill(2)}T23:59:59Z"
            return f"{yil}-{aylar.get(a1,'01')}-{str(g1).zfill(2)}T00:00:00Z"
        return None
    except: return None

def extract_dates(text): return format_tarih_iso(text, False), format_tarih_iso(text, True)

def get_category(text, title):
    t = tr_lower(title + " " + text)
    if any(x in t for x in ["trendyol","amazon","hepsiburada","n11","pazarama"]): return "Online AlÄ±ÅŸveriÅŸ"
    if "akaryakÄ±t" in t or "benzin" in t: return "YakÄ±t"
    if "market" in t or "gÄ±da" in t: return "Market"
    if "restoran" in t or "kafe" in t: return "Restoran & Kafe"
    if "giyim" in t or "moda" in t: return "Giyim & Moda"
    if "elektronik" in t or "teknoloji" in t: return "Elektronik"
    if "mobilya" in t: return "Ev & YaÅŸam"
    if "seyahat" in t or "uÃ§ak" in t or "otel" in t: return "Seyahat"
    return "DiÄŸer"

def extract_financials(text, title):
    t_low = tr_lower(text.replace('.', '').replace(',', '.'))
    title_low = tr_lower(title)
    min_s, max_d, earn, disc = 0, 0, None, None
    
    # Taksit
    if "taksit" in title_low:
        tm = re.findall(r'(\d+)\s*taksit', t_low)
        if tm:
            disc = f"{max(map(int, tm))} Taksit"
            ms = re.search(r'(\d+)\s*tl.*?taksit', t_low)
            if ms: min_s = int(ms.group(1))
        return min_s, None, disc, 0, 0

    # Toplam
    mt = re.search(r'toplam(?:da)?\s*(\d+)\s*(?:tl|worldpuan)', t_low)
    if mt: max_d = int(mt.group(1))

    # DÃ¶ngÃ¼sel
    mc = re.search(r'her\s*(\d+)\s*tl.*?(\d+)\s*tl', t_low)
    if mc:
        min_s = int(mc.group(1)); unit = int(mc.group(2))
        earn = f"{format_rakam(max_d if max_d else unit)} TL Worldpuan"
        return min_s, earn, None, 0, max_d
    
    # Tek seferlik
    mo = re.search(r'(\d+)\s*tl.*?(\d+)\s*tl\s*(?:worldpuan|indirim)', t_low)
    if mo:
        min_s = int(mo.group(1)); val = int(mo.group(2))
        if val != min_s:
            suff = "Ä°ndirim" if "indirim" in t_low else "Worldpuan"
            earn = f"{format_rakam(val)} TL {suff}"
            if not max_d: max_d = val

    return min_s, earn, disc, 0, max_d

def extract_cards(text):
    cards = []
    t = tr_lower(text)
    if "ticari" in t or "business" in t: cards.append("VakÄ±fBank Ticari")
    if "bankomat" in t: cards.append("Bankomat Kart")
    if "worldcard" in t or "bireysel" in t: cards.append("VakÄ±fBank Worldcard")
    if "platinum" in t: cards.append("Platinum")
    if "rail&miles" in t: cards.append("Rail&Miles")
    if not cards: cards.append("VakÄ±fBank KartlarÄ±")
    return list(set(cards))

# --- Ã‡ALIÅAN FONKSÄ°YONU (WORKER) ---
def worker_task(urls, worker_id):
    """Bu fonksiyon ayrÄ± bir tarayÄ±cÄ±da Ã§alÄ±ÅŸÄ±r ve kendine verilen URL listesini iÅŸler."""
    print(f"   ğŸ¤– Ä°ÅŸÃ§i #{worker_id} baÅŸladÄ±. ({len(urls)} kampanya iÅŸleyecek)")
    
    chrome_options = Options()
    # HÄ±z iÃ§in resimleri yÃ¼kleme ve Eager modunu kullan
    chrome_options.add_argument("--headless=new") # Arka planda Ã§alÄ±ÅŸsÄ±n (HÄ±zlÄ±)
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.page_load_strategy = 'eager' 
    prefs = {"profile.managed_default_content_settings.images": 2} 
    chrome_options.add_experimental_option("prefs", prefs)
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    results = []
    
    try:
        for i, url in enumerate(urls):
            try:
                driver.get(url)
                # BaÅŸlÄ±ÄŸÄ±n gelmesini bekle (Max 5 sn)
                try:
                    WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CSS_SELECTOR, "h1")))
                except: pass # Devam et

                soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                # Veri Ã‡ekme
                title_el = soup.select_one('.kampanyaDetay .title h1') or soup.select_one('h1')
                title = temizle_metin(title_el.text) if title_el else "BaÅŸlÄ±k Yok"
                
                img_el = soup.select_one('.kampanyaDetay .coverSide img')
                image = urljoin(BASE_URL, img_el['src']) if img_el else None
                
                content_div = soup.select_one('.kampanyaDetay .contentSide')
                conditions = []
                full_text = ""
                
                if content_div:
                    lis = content_div.select('li')
                    if lis:
                        conditions = [temizle_metin(li.text) for li in lis]
                    else:
                        ps = content_div.select('p')
                        conditions = [temizle_metin(p.text) for p in ps if len(p.text) > 15]
                    full_text = " ".join(conditions)
                
                # Analiz
                vf, vu = extract_dates(full_text)
                cat = get_category(full_text, title)
                min_s, earn, disc, _, max_d = extract_financials(full_text, title)
                cards = extract_cards(full_text)
                
                part = []
                if "cepte kazan" in tr_lower(full_text): part.append("Cepte Kazan")
                if "sms" in tr_lower(full_text): part.append("SMS")
                part_str = ", ".join(part) if part else "Otomatik / Detaylara bakÄ±n"
                
                desc = conditions[0] if conditions else title
                if len(desc) > 300: desc = desc[:300] + "..."

                item = {
                    "id": 0, # Sonra gÃ¼ncellenecek
                    "title": title,
                    "provider": IMPORT_SOURCE_NAME,
                    "category": cat,
                    "merchant": None,
                    "image": image,
                    "images": [image] if image else [],
                    "description": desc,
                    "url": url,
                    "discount": disc,
                    "earning": earn,
                    "min_spend": min_s,
                    "max_discount": max_d,
                    "created_at": datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ"),
                    "valid_from": vf,
                    "valid_until": vu,
                    "participation_method": part_str,
                    "conditions": conditions,
                    "eligible_customers": cards,
                    "source_url": BASE_URL
                }
                results.append(item)
                
            except Exception as e:
                print(f"      ! Hata (Worker {worker_id}): {e}")

    finally:
        driver.quit()
        print(f"   âœ… Ä°ÅŸÃ§i #{worker_id} gÃ¶revini tamamladÄ±.")
        
    return results

# --- ANA FONKSÄ°YON ---
def main():
    print(f"ğŸš€ {IMPORT_SOURCE_NAME} HÄ±zlÄ± TarayÄ±cÄ± BaÅŸlÄ±yor...")
    
    # 1. ADIM: Linkleri Topla (Tek TarayÄ±cÄ± ile HÄ±zlÄ±ca)
    print("\nğŸ“‹ AdÄ±m 1: Kampanya Linkleri ToplanÄ±yor...")
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    
    campaign_urls = []
    try:
        for page in range(1, 15):
            url = LIST_URL_TEMPLATE.format(page)
            driver.get(url)
            time.sleep(1.5)
            
            items = driver.find_elements(By.CSS_SELECTOR, "div.mainKampanyalarDesktop:not(.eczk) .list a.item")
            if not items: break
                
            for item in items:
                href = item.get_attribute('href')
                if href and href not in campaign_urls:
                    campaign_urls.append(href)
            print(f"   -> Sayfa {page} tarandÄ±. Toplam: {len(campaign_urls)}")
    finally:
        driver.quit()

    if not campaign_urls:
        print("âŒ Link bulunamadÄ±.")
        return

    # 2. ADIM: Ä°ÅŸ BÃ¶lÃ¼mÃ¼ ve Paralel Ã‡alÄ±ÅŸtÄ±rma
    print(f"\nâš¡ AdÄ±m 2: {len(campaign_urls)} kampanya {WORKER_COUNT} iÅŸÃ§iye bÃ¶lÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
    
    # URL listesini iÅŸÃ§i sayÄ±sÄ±na bÃ¶l
    chunk_size = math.ceil(len(campaign_urls) / WORKER_COUNT)
    chunks = [campaign_urls[i:i + chunk_size] for i in range(0, len(campaign_urls), chunk_size)]
    
    final_data = []
    
    with ThreadPoolExecutor(max_workers=WORKER_COUNT) as executor:
        futures = []
        for i, chunk in enumerate(chunks):
            futures.append(executor.submit(worker_task, chunk, i+1))
        
        for future in futures:
            final_data.extend(future.result())

    # ID'leri dÃ¼zenle ve Kaydet
    for i, item in enumerate(final_data, 1):
        item['id'] = i
        
    if final_data:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            json.dump(final_data, f, ensure_ascii=False, indent=4)
        print(f"\nğŸ‰ Ä°ÅLEM BÄ°TTÄ°! {len(final_data)} kampanya '{OUTPUT_FILE}' dosyasÄ±na kaydedildi.")
    else:
        print("\nâŒ Veri Ã§ekilemedi.")

if __name__ == "__main__":
    main()